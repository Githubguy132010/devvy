# LLM Provider Configuration Example
# Copy this file to .env and add your API keys

# OpenAI Configuration
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic (Claude) Configuration
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google (Gemini) Configuration
# Get your API key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# Ollama Configuration (Local Models)
# No API key needed for Ollama - runs locally
# Install from: https://ollama.ai/
# Default base URL: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# Note: You only need to configure the providers you plan to use.
# API keys are stored locally and never leave your machine unless
# you use them to make API calls to the respective services.
